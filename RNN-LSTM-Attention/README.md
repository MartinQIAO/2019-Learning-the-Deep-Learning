# Objective of this repo
## Sort out the technical development journey clearly for Sequential Modeling problem.

I've been reading some papers here and there related to Sequential Modeling problem, and also some blogs on CSDN and Zhihu (Chinese version of Quora) trying to explain their understanding of the RNN/LSTM/Attention model. However till now, none of these framented articles could explain the detailed journey of the model design clearly, especially regarding the granularity of the knowledge that required for someone who wants to be a full-stack Deep Learning engineer (I mean the level of details which could meet the requirement of interview by nVidia or Google's AI team.) Therefore I decided to do this in person to sort out how researchers' thinking and design of the Sequential Model changes along the journey, and specifically what happens in the coding level for each bit of the neural network. I could foresee this is not gonna be a easy task for me but I do believe this would be the right thing to do for my career. Ane hopfully this could help someone who have been not able to drill into details of understanding this journey due to the nature of their job. 

>*Remark: I used to be PhD researcher on imaging science when deep learning has not become a hype. However due to the current job nature, I have been busy on putting up presentation slides for project management and explaining fundamental stuff to senoir business managers as my daily job which means I would not be able to get the deep-dive knowledge through on-the-job training.


# Structure of this repo

## 1. Recommended reading materials

### Journey of the model design: RNN --> Seq2Seq --> LSTM --> Attention --> Transformer

## 2. My own interpretation and notes
## 3. Examples to get hands dirty
