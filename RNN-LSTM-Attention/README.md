# Objective of this repo
## Sort out the technical development journey clearly for Sequential Modeling problem.

I've been reading some papers here and there related to Sequential Modeling problem, and also some blogs on CSDN and Zhihu (Chinese version of Quora) trying to explain their understanding of the RNN/LSTM/Attention/Transformer model. However till now, none of the single article could explain the detailed journey of the model design clearly, especially at the granularity of the knowledge that required for someone who wants to be a full-stack Deep Learning engineer. Therefore I decided to do this in person to sort out how researchers' thinking and design of the Sequential Model changes along the journey, and specifically what happens in the coding level for each bit of the neural network. I could foresee this is not gonna be a easy task for me but I do believe this would be the right thing to do for my career. Ane hopfully this could help someone who have been not able to drill into details of understanding this journey due to the nature of their job. 

>*Remark: I used to be PhD researcher on imaging science when deep learning has not become a hype. However due to the current job nature, I have been busy on putting up presentation slides for project management and explaining fundamental stuff to senoir business managers as my daily job which means I would not be able to get the deep-dive knowledge through on-the-job training. I believe there will be people in similar situations where I do hope my study could be useful for them to use it as a short cut to understand the tricks for sequential deep learning model.


# Structure of this repo

## 1. My notes

### Journey of the model design: RNN --> Seq2Seq --> LSTM --> Attention --> Transformer


## 2. ecommended reading materials

## 3. Examples to get hands dirty
